
Intrinsic Lock: implemented by the synchronized keyword, unfair lock
Explicit Lock: implemented by the implementation class of the java.concurrent.locks.Lock interface (such as the java.concurrent.locks.ReentrantLock class), which supports both fair locks and unfair locks.

ReentrantLock lock = new ReentrantLock(boolean fair) // true indicates fair lock (default is unfair scheduling)

The acquisition of the lock implies a refresh of the processor cache load from memory to cache
Releasing the lock implies flushing the processor cache to memory.

Memory Reordering

Reentrancy: : Whether a thread can re-apply for a lock (or multiple times) while it holds the lock
If a thread can continue to successfully apply for a lock while holding it, then we call the lock reentrant, otherwise we call the lock non-reentrant.

Each object has a unique lock associated with it. This lock is called a monitor or intrinsic lock.

Volatile usage scenarios:
1. Use volatile variables as status flags. In this scenario, a state of the application is set by one thread, and other threads read the state and use it as the basis for their calculations.
2. Use volatile to ensure visibility. In this scenario, multiple threads share a mutable state variable. When one thread updates the variable, other threads can see the update without locking.
3. Use volatile variables instead of locks. The volatile keyword is not a substitute for locks, but it is more suitable than locks under certain conditions (low performance overhead and simple code)

caveat: The volatile keyword can only work on operations on the array reference itself, but not on operations on array elements (reading or updating array elements).
If you want the read and write operations on array elements to also trigger the effect of the volatile keyword, you can use AtomicIntegerArray, AtomicLongArray and AtomicReferenceArray.

AtomicReference conditionally updates reference variables: When updating a reference variable, make sure that the variable is indeed the one we want to modify, that is, the variable has not been modified by other threads.

A thread can call the wait method of an object only when it holds the internal lock of the object, so the Object.wait() call is always placed in the critical section led by the corresponding object.
The method that describes the template code is called a protected method.
Since the same method of the same object (someObject.wait()) can be executed by multiple threads, there may be multiple waiting threads for an object.

synchronized(someObject){
while (protection condition is not met) {
someObject.wait(); // Call Object.wait() to pause the current thread and release the internal lock corresponding to someObject
}
// The code execution here indicates that the protection condition has been met
// Execute the target action
doAction();
}

synchronized(someObject){
updateSharedState(); // Update the shared variables involved in the protection conditions of the waiting thread
someObject.notify(); // Waking up other threads can only execute the notify method of an object when holding the internal lock of the object
}

The lock will be released only after the critical section code where Object.notify() is called is executed, and Object.notify() itself will not release the internal lock. Generally, place the Object.notify() call as close to the end of the critical section as possible.
The place


java.util.concurent.locks.Condition: The Condition interface can be used as a substitute for wait/notify to implement wait/notify, which can solve the problem of premature wakeup and the problem that Object.wait(long) cannot distinguish whether its return is due to a wait timeout.
Caused by the problem.

Condition instances are also called condition variables or condition queues.
Each Condition instance maintains a queue for storing waiting threads.
cond1.signal() will wake up any thread in the waiting queue of cond1.
cond1.signalAll() will wake up all threads in the waiting queue of cond1.

private final Lock lock = new ReentrantLock();
private final Condition condition1 = lock.newCondition(); // condition1 is generated by display lock
private final Condition condition2 = lock.newCondition(); // The same lock can generate multiple conditions for precise wakeup (the protection conditions of each thread are different)

lock.lock();
try {
while (guard condition is not met) {
condition.await();
}
doAction(); // Execute the target action
finally
lock.unlock();
}

lock.lock();
try {
changeState(); // Update shared variables
condition.signal();
finally
lock.unlock();
}

Condition.awaitUntil (Date deadline) returns true if the deadline has not yet arrived


CountdownLatch: When you need to wait for a specific operation performed by other threads to complete, without having to wait for these threads to terminate.

Blocking Queue vs Non-blocking Queue
Blocking Queue: threads need to wait for the queue's availability. Producer waits for available space, Consumer waits for available item.
Non-blocking Queue: threads don't wait because queue throws an exception or returns a special value (null or false)

Blocking:
ArrayBlockingQueue: bounded queue, has a fixed size, a single lock for put and take. Underlying is an array
LinkedBlockingQueue: unbounded queue (Integer.MAX_VALUE), can set capacity, put and take have separate lock. Underlying is linked list
PriorityBlockingQueue: take operation can occur simultaneously with the put operation (uses spinlock)
DelayQueue: leader thread + conditional variable + PriorityQueue + sleep.

Non-blocking:
ConcurrentLinkedQueue: add and poll are guaranteed to be thread-safe and return immediately, uses CAS instead of lock
=========

Spinlock: When a thread is acquiring a lock, if the lock has been acquired by another thread, the thread will wait in a loop and then continue to determine whether the lock can be successfully acquired until the lock is acquired.
(1) Spin locks do not cause the thread state to switch, and the thread remains in user state, that is, the thread is always active; they do not cause the thread to enter a blocked state, thus reducing unnecessary context switching and increasing execution speed.
(2) When a non-spin lock cannot acquire the lock, it will enter a blocked state and then enter the kernel state. When the lock is acquired, it needs to be restored from the kernel state, which requires a thread context switch.
(After the thread is blocked, it enters the kernel (Linux) scheduling state, which causes the system to switch back and forth between user state and kernel state, seriously affecting the performance of the lock)


//Threadpool in java
ExecutorService executorService = Executors.newFixedThreadPool(10); // set up a thread pool with 10 threads.
Futurefuture = executorService.submit(() -> "Hello World");
String result = future.get(); // wait for task to complete

==========
The following syntax is often seen in the Java concurrent library:
public void someMethod() {
final ReentrantLock lock = this.lock;
lock.lock();
...
}

Why do this?

Reason 1: To speed up access;
Assign the global variable to a local variable of the method. When accessing, directly access it from the thread stack, which is faster than accessing member variables. Reading the variable in the stack only requires one instruction, while reading the member variable requires two instructions.

The second reason is for safety;
If you just want fast access speed, then you can directly use a normal local variable without adding final. The reason for adding final is to ensure thread safety under multi-threading.
The role of final is that once initialized, it cannot be changed, and it guarantees the memory reordering of object access and the visibility of the object. For more details, see here
————————————————
Disadvantages of CAS:

(1) Only the atomicity of one shared variable can be guaranteed
CAS is not like synchronized and RetranLock, which can guarantee the synchronization of a piece of code and multiple variables. CAS cannot guarantee the operation of multiple shared variables, so locking must be used to achieve it.

(2) There is a performance overhead problem
Since CAS is a spin operation, if CAS fails for a long time, it will cause great overhead to the CPU.

(3) ABA Problem
Because CAS ensures atomicity by checking whether the value has changed, if the value of a variable V is A, and thread 1 and thread 2 both read the value A of this variable at the same time, thread 1 changes the value of V to B, and then changes it back to A. During this period, thread 2 has not been able to grab the CPU time slice. Thread 2 is not executed until thread 1 changes the value of V back to A. At this time, thread 2 does not know that the value of V has changed. This problem is called the ABA problem.
The solution to the ABA problem is actually easy to handle, that is, add a version number, and update the version number at the same time as the update value. The AtomicStampedReference mentioned above is used to solve the ABA problem.

Link: https://juejin.cn/post/6977993272538955806
---------------------
Semaphore: We can use semaphores to limit the number of concurrent threads accessing a specific resource.
constructors:  
Semaphore(int num)
Semaphore(int num, boolean isFair) // permit is released to the thread that has been waiting for longest time.
how to use:
semaphore.acquire() // try to acquire the permit
semaphore.release()

    semaphore.acquire(int permits)
    semaphore.release(int permits)

Conditional Variable:
Lock lock = new ReentrantLock();
Condition cv = lock.newCondition();
cv.await(); // is always used with lock.lock()/unlock() The judgment condition must be protected by lock, otherwise it will be signaled in advance and wait infinitely, see: https://zhuanlan.zhihu.com/p/55123862
cv2.signal(); cv2.signalAll();

The signal() method must not be used unless all of these conditions are met:
(1) The Condition object is identical for each waiting thread.
(2) All threads must perform the same set of operations after waking up, which means that any one thread can be selected to wake up and resume for a single invocation of signal().
(3) Only one thread is required to wake upon receiving the signal.
or all of these conditions are met:
(1) Each thread uses a unique Condition object.
(2) Each Condition object is associated with the same Lock object.
When used securely, the signal() method has better performance than signalAll().

signalAll() is a method specific to a Condition, whereas notifyAll() is done on any object you're locking on.
signalAll() should be used when you're waiting/sleeping with Condition.await(), and notifyAll() when you're using Object.wait() inside a synchronized block.

One of the drawback of using Future is that you either need to periodically check whether the task is completed or not
e.g. by using isDone() method or wait until task is completed by calling blocking get() method.
There is no way to receive the notification when the task is completed.
This shortcoming is addressed in CompletableFture, which allows you to schedule some execution when the task is done.
CompletableFuture class is introduced in Java 8 and you can perform some task when Future reaches completion stage
Read more: https://javarevisited.blogspot.com/2015/01/how-to-use-future-and-futuretask-in-Java.html#ixzz6rxGbLU9c


[Notes from daily work]:
1. concurrent set allows concurrent iteration (In other words, it can safely iterate while other thread is attempting to modify this set)

[reference]:
Basics: http://www.10tiao.com/html/689/201804/2651581230/1.html
Promotion: https://blog.csdn.net/gitchat/article/details/79983445
Advanced 1: http://www.10tiao.com/html/689/201805/2651581519/1.html
Advanced 2: http://gitbook.cn/books/5ac70a26d60a134e37dafdd7/index.html
Advanced Article 3: https://blog.csdn.net/valada/article/details/79910098
/****************************************************************************************************************/
1. The heap is the largest piece of memory in a process. The heap is shared by all threads in the process and is allocated when the process is created. The heap mainly stores object instances created using the new operation.
2. The method area is used to store code snippets in the process and is shared by threads.
3. There are three thread creation methods in Java, namely
   (1) Implement the runnable method of the Runnable interface; // no return value
   (2) Inherit the Thread class and override the run method; // no return value
   (3) Use FutureTask method. // allows to return value
   //Create a task class, similar to Runable
   public static class CallerTask implements Callable
   {        
   @Override
   public String call() throws Exception {            
   return "hello";
   }
   }  
   public static void main(String[] args) throws InterruptedException {    
   FutureTask
   futureTask = new FutureTask<>(new CallerTask()); // Create an asynchronous task      
   new Thread(futureTask).start(); //Start thread
   try {          
   String result = futureTask.get(); //Wait for the task to be completed and return the result
   System.out.println(result);
   } catch (ExecutionException e) {
   e.printStackTrace();
   }
   }
4. wait() method:
   When a thread calls the wait() method of a shared object, the calling thread is blocked and does not return until one of the following things happens:
   (1) Other threads call the notify() or notifyAll() method of the shared object;
   (2) Other threads call the interrupt() method of this thread to set the interrupt flag of this thread. This thread will throw an InterruptedException and return an exception.
   [Difference between thread interrupted() and isInterrupted()]:
   In addition to returning the interrupt flag, interrupted() also clears the interrupt flag (that is, sets the interrupt flag to false);
   isInterrupted() simply returns the interrupt flag;
   That is, when the interrupted() function is called, the interrupt flag will temporarily become true and then be clearly changed to false.

5. Spurious wakeup
   If the thread is not awakened by other threads calling { notify(), notifyAll(), or being interrupted, or waiting for timeout}, this wakeup is called a spurious wakeup.
   [Prevent spurious wakeup]:
   Use a loop to continuously test whether the conditions for the thread to be awakened are met. If not, continue to wait:
   synchronized (obj) { // Note that this obj is usually modified with private final and cannot be changed: private final Object lock = new Object();
   while (condition not met){
   obj.wait();  
   }
   }
6. void notify() method:
   It will wake up a thread that is suspended after calling the wait series of methods on the shared variable. There may be multiple threads waiting on a shared variable, and which waiting thread is woken up is random.
   void notifyAll() method:
   Will wake up all threads that are suspended due to calling the wait series of methods on the shared variable.
   [Difference between notify and notifyAll()]:
   (1) Wake-up does not mean execution. Wake-up -> get monitor -> execute.
   (2) For example: thead1 and thread2 both call obj.wait() to block themselves, and thread3 is in the active state.
   If obj.notify() is called, if thread1 is awakened and completes execution after getting the monitor, even if thread1 completes execution and releases the monitor, thread2 is still blocked because it has not been awakened.
   If obj.notifyAll() is called, both thread1 and thread2 will be awakened. After thread1 gets the monitor and finishes executing, thread2 will get the monitor and continue executing.

7. join() method:
   Block yourself and wait until some threads are finished before continuing to execute
   thread1.join(); // Wait until thread1 is finished before this thread can execute
   /****************************************************************************************************************/
1. Visibility failures of shared variables
   Thead A | Thead B
   ----------------------|------------------------
   Control Unit | Control Unit
   ALU | ALU
   L1 Cache | L1 Cache
   ----------------------|------------------------
   L2 Cache (shared)
   -----------------------------------------------
   Main Memory (shared variables)
   -----------------------------------------------
   Concept: Thread working memory = Registers + L1 Cache + L2 Cache
   Example: Assume that threads A and B use different CPUs to modify the shared variable X. Assume that X is initialized to 0 and both levels of cache are empty. See the following analysis for details:
   (1) Assume that thread A first obtains the shared variable X. Since neither of the two levels of cache hits, it loads X=0 from the main memory and then caches the value of X=0 in the two levels of cache. Assume that thread A modifies X=1, writes it to the two levels of cache, and refreshes it to the main memory (Note: if the main memory is not refreshed, there will also be a memory invisible problem).
   (2) At this time, the value of X in both the two-level cache and the main memory of the CPU where thread A is located is 1;
   (3) Then suppose that thread B obtains the value of X at this time. First, the first-level cache does not hit, then it checks the second-level cache. The second-level cache hits, so X=1 is returned; then thread B modifies the value of X to 2; then it is stored in the first-level cache where thread 2 is located and the shared second-level cache, and finally the main memory value is updated to 2;
   (4) Then suppose that thread A needs to modify the value of X again this time. When obtaining it, it hits the first-level cache and obtains X=1. Here, a problem arises. Thread B has obviously changed the value of X to 2, so why does thread A still obtain 1?
   This is the memory invisibility problem of shared variables, that is, the value written by thread B is not visible to thread A.

2. Synchronized:
   Synchronized block is an atomic built-in lock provided by Java. Every object in Java can be used as a synchronized lock. These built-in locks in Java that are invisible to users are called internal locks, also called monitor locks.
   There are three situations in which the Synchronized block releases the lock:
   (1) Normally exit the synchronized code block
   (2) After an exception is thrown
   (3) The wait() method of the built-in lock resource is called in the synchronized block

   How does Synchronized solve the problem of invisible memory?
   (1) When a thread enters a synchronized block, the variables used in the synchronized block are cleared from the thread's working memory. When the variable is used, it is not obtained from the thread's working memory, but directly from the main memory;
   (2) When exiting the Synchronized block, the changes to the shared variables in the Synchronized block will be refreshed to the main memory;

   remark: The Synchronized keyword will cause thread context switching and thread scheduling overhead; Synchronized can solve the problem of memory invisibility and implement atomic operations

   For Synchronized, every object in Java can be used as a lock, which has the following three forms:
   (1) synchronized ordinary method, the lock is the current instance object
   (2) synchronized static method, the lock is the current class Class object
   (3) synchronized code block, the lock is the object configured in the Synchronized brackets
   (4) Synchronized code block, such as synchronized (PersonService.class), the lock is all objects of the class in the synchronized brackets

3. Volatile:
   Once a variable is modified by volatile, when a thread obtains the value of this variable, it will first clear the value of the variable in the thread's working memory, and then obtain the value of the variable from the main memory;

4. Differences and similarities between sleep and wait:
   Similarities: Both sleep and wait release the right to use the CPU
   Difference: sleep does not release the lock, while wait releases the lock, allowing other threads to use synchronization methods or synchronization control blocks
   wait, notify and notifyAll can only be used in synchronization control methods or synchronization control blocks, while sleep can be used anywhere (scope of use)

5. What is false sharing, why does it occur, and how to avoid it
6. What are reentrant locks, optimistic locks, pessimistic locks, fair locks, unfair locks, exclusive locks, and shared locks?
7. The difference between Reentrant Lock and Syncronized

/****************************************************************************************************
* "Java Concurrency Programming Practice" *
  *********************************************************************************************************/
  Chapter 2 Thread Safety:
1. Stateless objects are always thread-safe
2. Compound operation: such as "read-modify-write" and "check before executing"
3. To ensure the consistency of the state, all relevant state variables need to be updated in a single atomic operation
4. Java built-in lock: synchronized block = lock object reference + lock protected code block
   Every Java object can be used as a lock, called a monitor lock or an intrinsic lock.
5. Reentrancy
   Java built-in locks are reentrant. If a thread attempts to acquire a lock that it already holds, the request succeeds.
   "Reentrancy" means that the granularity of acquiring locks is thread rather than invoke. Each lock is associated with a count value and an owner thread. When the same thread acquires the lock again, the count value increases; when the thread exits the synchronized code block, the count value decreases, and when it is equal to 0, the lock is released.
6. When performing calculations or operations that take a long time (for example, network I/O, console I/O), do not hold locks.

Chapter 3 Object Sharing
1. Memory visibility: To ensure that the results of write operations are visible to multiple threads, a synchronization mechanism must be used
2. Reordering: Without synchronization, the compiler and processor may make unexpected adjustments to the order of execution.
3. Non-atomic 64-bit operations: JVM allows 64-bit read and write operations to be split into two 32-bit operations. If the 64-bit read and write operations are performed in two threads, the high 32 bits of a value and the low 32 bits of another value will be read.
4. Accessing volatile variables will not perform locking operations and will not block threads, so it is a lighter synchronization mechanism than synchronized.
5. Locking is not limited to mutual exclusion, but also includes memory visibility. To ensure that all threads see the latest value of a shared variable, all threads performing read and write operations must synchronize on the same lock.
6. Locking can ensure both visibility and atomicity. Volatility can only ensure visibility.
   Currently, you should use volatile variables only when all of the following conditions are met:
   (1) Variable write operations do not depend on the current value of the variable
   (2) This variable will not be included in the invariance condition with other state variables
   (3) No locking is required when accessing variables
7. Publish: The object can be used in code blocks outside the current scope
   Escape: When an object is released that should not be released

/****************************************************************************************************
* The Art of Java Concurrency Programming *
  *********************************************************************************************************/
1. Context Switch Measurement Tool:
   (1) The duration of Lmbench3 context switching;
   (2) vmstat can measure the number of context switches;

2. Java object header
   The lock used by synchronize is stored in the Java object header.
   <1> Array type, JVM uses 3 words to store the object header (Mark Word, Class Metadata Address, ArrayLength)
   <2> For non-array types, JVM uses 2 words to store the object header (Mark Word, Class Metadata Address)

   Mark Word storage: (1) HashCode, (2) Generation age, (3) Lock mark

3. Lock upgrade
   Unlocked state --> Biased lock state (Biased Lock) --> Lightweight lock state --> Heavyweight lock state
   (1) As competition gradually escalates
   (2) Locks can be upgraded but not downgraded

4. Several concepts
   (1) cache line //The smallest operation unit of the cache, and also the smallest storage unit that the cache can allocate
   (2) Memory barriers //A set of processor instructions used to restrict the order of memory operations
   (3) Memory order violation //Memory order violation, caused by false sharing. When a memory order violation occurs, the CPU must clear the pipeline.
   (4) False sharing // False sharing means that multiple CPUs modify different parts of the same cache line at the same time, causing the operation of one CPU to be invalid
   (5) CAS = Compare And Swap // Requires two values ​​<expected value/old value, updated value>. Compare the expected value and the current value to see if they are consistent. If they are consistent, replace them with the updated value; otherwise, do not replace. This is done with one machine instruction

5. There are two ways to implement atomic operations between multiple processors
   (1) Bus Lock
   (2) Cache Lock

6. ReentrantReadWriteLock
   The prerequisite for a thread to enter the read lock is: (1) no other thread has the write lock, and (2) there is no write request or there is a write request, but the calling thread and the thread holding the lock are the same.
   The prerequisite for a thread to enter the write lock is: (1) no other thread has a read lock, and (2) no other thread has a write lock
   Properties of ReadLock and WriteLock:
   (a) In terms of reentrancy, the internal WriteLock can obtain the ReadLock, but conversely, the ReadLock should never be able to obtain the WriteLock.
   (b). WriteLock can be downgraded to ReadLock. The order is: first obtain WriteLock, then obtain ReadLock, and then release WriteLock. At this time, the thread will keep holding ReadLock. Conversely, it is impossible for ReadLock to be upgraded to WriteLock. Why? See (a).
   (c) ReadLock can be held by multiple threads and exclude any WriteLock when in effect, while WriteLock is completely mutually exclusive. This feature is the most important because for data structures with high read frequency and relatively low write frequency, using this type of lock synchronization mechanism can increase concurrency.
   (d) Both ReadLock and WriteLock support Interrupt, and their semantics are consistent with ReentrantLock.
   (e)WriteLock supports Condition and has the same semantics as ReentrantLock, while ReadLock cannot use Condition, otherwise it will throw an UnsupportedOperationException
   Q: Why can't I obtain WriteLock when ReadLock is reentered?
   A: Because ReadLock is a shared lock, it may be held by another thread besides the current thread. Note that one of the prerequisites for a write lock is (1) that no other thread has a read lock. Imagine that if the current thread obtains the write lock, and another thread
   When holding a read lock, inconsistent

(For various Java locks, please refer to the blog of Meitu Technology Team https://tech.meituan.com/2018/11/15/java-lock.html )

     
    
   
